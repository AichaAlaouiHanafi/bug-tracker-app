import streamlit as st
import pandas as pd
import numpy as np
import os
import requests
from sentence_transformers import SentenceTransformer, util

# === Chemins ===
# === Chemins relatifs au dossier du projet ===
BASE_DIR = os.path.dirname(__file__)  # dossier o√π se trouve streamlit_app.py

DATA_PATH = "src/data_cleaned_fusion_concat.xlsx"
EMB_PATH = "src/embedding.npy"
MODEL_PATH = "src/all-MiniLM-L6-v2"

GEMINI_API_KEY = "AIzaSyAAv2LbIhkkk2gGVLJwTupjl5GMxHWVFNw"

# === Chargement des donn√©es ===
df = pd.read_excel(DATA_PATH)
model = SentenceTransformer(MODEL_PATH)
embeddings = np.load(EMB_PATH, allow_pickle=True)

# === Fonction : D√©tection du type de probl√®me ===
def detect_type(text):
    incident_keywords = ["incident", "no data", "erreur", "crash", "failed", "ko", "indisponible", "interruption", "injoignable"]
    bug_keywords = ["bug", "anomalie", "d√©faut", "incorrect", "probl√®me", "affichage", "port", "valeur erron√©e"]

    text = text.lower()
    if any(kw in text for kw in incident_keywords):
        return "incident"
    elif any(kw in text for kw in bug_keywords):
        return "bug"
    else:
        return "bug"

# === Fonction IA : G√©n√©ration de solution avec Gemini ===
def generate_ai_solution(prompt):
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={GEMINI_API_KEY}"
    if not prompt.strip():
        return "Aucun prompt fourni pour la g√©n√©ration de solution."
    if not GEMINI_API_KEY:
        return "Cl√© API Gemini manquante."
    headers = {"Content-Type": "application/json"}
    data = {
        "contents": [
            {
                "parts": [
                    {
                        "text": f"Propose une solution au probl√®me suivant : {prompt}"
                    }
                ]
            }
        ]
    }
    try:
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            return response.json()["candidates"][0]["content"]["parts"][0]["text"]
        else:
            return f"Erreur API Gemini : {response.status_code} - {response.text}"
    except Exception as e:
        return f"Erreur lors de l'appel API : {e}"

# === Interface ===
st.title(" Assistant de Suivi des Bugs Internes")
desc = st.text_area(" D√©cris le probl√®me rencontr√© ", key="bug_description")

# === D√©tection et affichage du type de probl√®me ===
if desc.strip():
    detected_type = detect_type(desc)
    if detected_type == "incident":
        st.error(" Ce probl√®me est un **incident**.")
    else:
        st.warning(" Ce probl√®me est un **bug**.")

# === Recherche de similarit√© ===
if st.button(" Rechercher"):
    if desc.strip():
        query_emb = model.encode([desc])
        scores = util.cos_sim(query_emb, embeddings)[0].cpu().numpy()
        top_idx = np.argsort(scores)[::-1][:5]
        results = df.iloc[top_idx].copy()
        results["similarit√©"] = scores[top_idx]

        st.session_state['last_results'] = results
        st.session_state['new_bug'] = desc
        st.session_state['detected_type'] = detected_type  # stocke le type

# === Affichage des r√©sultats ===
if 'last_results' in st.session_state:
    st.subheader(" R√©sultats similaires :")
    st.dataframe(
        st.session_state['last_results'][["Summary", "similarit√©", "Resolution"]]
        if "Resolution" in st.session_state['last_results'].columns
        else st.session_state['last_results']
    )

    # üîç R√©solution trouv√©e ?
    top_resolution = st.session_state['last_results'].iloc[0].get("Resolution", "")
    if isinstance(top_resolution, str) and top_resolution.strip():
        st.success(f" Solution propos√©e depuis la base : {top_resolution}")
    else:
        st.warning(" Aucune solution trouv√©e. G√©n√©ration via l'IA...")
        ai_solution = generate_ai_solution(st.session_state['new_bug'])
        st.info(f" Suggestion IA : {ai_solution}")
        st.session_state['ai_solution'] = ai_solution

    # ‚ûï Ajout du bug
    with st.expander(" Ajouter ce bug comme nouveau"):
        default_solution = st.session_state.get('ai_solution', "")
        resolution = st.text_area("Ajouter/modifier une solution", value=default_solution, key="solution_input")
        if st.button("Confirmer l‚Äôajout"):
            new_row = {
                "Summary": st.session_state['new_bug'],
                "Issue key": "NEW-" + str(len(df) + 1),
                "Issue Type": st.session_state['detected_type'],  # type ici
                "Resolution": resolution,
                "Type": st.session_state['detected_type']  # colonne 'Type' dans Excel
            }
            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
            try:
                df.to_excel("data_cleaned.xlsx", index=False, engine="openpyxl")
                st.success(" Nouveau bug ajout√© avec succ√®s !")
                st.dataframe(pd.DataFrame([new_row]))
            except Exception as e:
                st.error(f" Erreur lors de la sauvegarde : {e}")

# üîÅ R√©entra√Ænement des embeddings
if st.button(" R√©entra√Æner les embeddings"):
    with st.spinner(" G√©n√©ration des nouveaux embeddings..."):
        os.system("python generate_embeddings.py")
        embeddings = np.load(EMB_PATH, allow_pickle=True)
    st.success(" Embeddings mis √† jour avec succ√®s !")

# üìä Infos sur le mod√®le
st.sidebar.header("‚ÑπInfos mod√®le")
st.sidebar.write("Mod√®le :", MODEL_PATH)
st.sidebar.write("Taille dataset :", len(df))
st.sidebar.write("Embeddings shape :", embeddings.shape)

# === Fin ===
